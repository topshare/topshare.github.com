---
layout: post
title: "ceph学习和观察"
description: "外行看热闹，外行看门道。在高铁上无聊之余看看最近Ceph社区的一些活动。可以通过社区的变化看到一些Ceph的方向。"
category: "TC"
tags: [Ceph, Storage]
---
{% include JB/setup %}

Ceph这样一个项目，短短几年时间就发展成了现在家喻户晓的分布式存储的典型代表，不得不说真的世道变化太快。记得在很早之前用Lustre和MooseFS的年代，感觉就已经很幸福了。最近有好多项目和朋友都问我Ceph如何如何的？为了不误导大家，我只能去多学习学习Ceph。下面是对最近Ceph社区活动的PPT的一个简单的总结，可能对很多问我Ceph的朋友有一些帮助。

## 社区的变化

如果你仔细去看看Ceph这半年的变化，你不得不承认整个热度和社区交流的质量有很大的飞跃。先从Ceph中国社区的角度来看看现在的情况如何：

### 2015年9月 Ceph中国社区线下活动

* Ceph中国社区介绍(只有可怜的几页PPT）
* 合作伙伴(10以内)
* 社区活动范围（帝都）
* 交流内容比较单一

### 2015年12月 Ceph中国社区线下活动

* Ceph中国社区介绍（PPT高大上不少）
* 微信助阵还是传播的快（移动互联网时代的产物）
* 合作伙伴（20+）
* 社区活动范围（帝都、魔都）
* 不少实战经验分享

看完热闹，吐一下槽，通过对比12月帝都和魔都的PPT交流的内容，可以比较明显的感受到帝都Ceph技术的积累就像北京的霾一样醇厚。整个交流从PPT的信息量来看，帝都的同学们还是有更多的积累（作为魔都的有生力量，我们还需努力）。

## 交流看点

这么多场交流，虽然没能现场参加，但PPT也可以学到很多东西。从整体上来看，大家都好像和OpenStack扯上了关系。OpenStack和Ceph就像一对情侣纠缠不清，就差明媒正娶。那亮点在哪，可以为各位看官什么有用的信息呢？

### 看点1 Ceph是否合适做对象存储

Ceph的强项好像一直是它的Block Stroage，但从乐视云的交流来看，其支撑的对象存储后端就采用Ceph。Ceph做对象存储，而且存储的数据量和文件数量还不小。从这点上来看未来Ceph好像要通吃全部存储的节奏（但个人感觉还是专一点好）。从公开的数据来看单个集群可以上PB级以上，集群总共有10PB左右。从这个描述来看，应该在rados gateway这块做了一些工作。看样子未来作为一个对象存储还是挺有戏的。但心中始终有个疑问，Swift和Ceph做对象存储到底是哪个更好（估计有朋友会说看业务场景）。

### 看点2 Ceph性能调优

技术交流的PPT中看到了很多公司在做性能的测试和调优（看样子还不敢直接给客户用）。从测试结果和调优过程，看到了一些门道，貌似大家都参考Redhat的调优方法（看来Redhat这块的投入还是挺大的）。从测试数据来看，通过大家的调优，性能貌似有很大的提升，但具体到生产上调优还是需要有更多的运维经验。


### 看点3：Ceph外围工具

Ceph部署和运维工具也是整个Ceph交流的热点，也看到了各种工具百花齐放的年代的到来（想想当年部署Ceph，也是醉了）。从Intel的VSM管理工具，到用SaltStack、ansible、puppet等Ceph的运维管理，只能说现在的Ceph更加趋于成熟。随着外围工具的不断完善，Ceph也更便于使用和维护，这也为中国很多创业公司创造了很多机会。

## 几点思考

* Ceph这样的分布式存储的存储备份如何考虑？
* OpenStack中Ceph的集成度真的好吗？
* Ceph未来是统一存储，还是专注于Block Storage？

## 总结

Ceph的不断发展就像OpenStack的发展速度，甚至在某些情况下其发展速度更快（但比起Docker，好像两个都逊色不少）。从一个多年OpenStack从业者的角度来看，对于企业级存储市场，Ceph是时候介入IaaS这部分，但还需要开发和运维人员勤勤恳恳的劳作。



